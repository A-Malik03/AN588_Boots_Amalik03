---
title: "Peer Comments 05"
author: "Erin Anderson"
date: "`r Sys.Date()`"
output: html_document
---


## Part 1
</br>Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).</br>

First, lets load in necessary packages
```{r load in packages}
library(ggplot2)
library(curl)
library(lmodel2)
```

```{r grab data}
KC <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
KC <- read.csv(KC, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(KC)#Check if its loaded in correctly
```
```{r, linear regression}
#Nice! Its loaded in, now lets make the linear regression of log(HomeRange_km2) in relation to log(Body_mass_female_mean). . . seems easy enough
KClin <- lm(log(HomeRange_km2) ~ log(Body_mass_female_mean), data = KC) #linear regression model y ~ x, homerange in realtion to body mass
summary(KClin) #check in on our linear regression
KClin$coefficients
```
</br>Based on the linear regression performed above, the β1, or slope, of the linear regression is 1.036432 and the β0, or the intercept, of the regression is -9.441231. So there is a positive relationship between HomeRangekm_2 and Body_mass_female_mean.</br>

```{r plot it}
#Lets try plotting it to better visualize the data
x <- log(KC$Body_mass_female_mean)
y <- log(KC$HomeRange_km2)
#Set the two variables to a simpler name, object x and y, for my variables
g <- ggplot(data = KC, aes(x = x, y = y)) 
g <- g + geom_point() #scatter plot the data
g <- g + labs(x = "Mean Female Body Mass", y = "Home Range(km2)") #label them because just having them as x and y
g <- g + geom_smooth(method = "lm", formula = y ~ x) #add the regression line
g #lets see it
```
## COMMENT: Looks great! 
## Part 2
</br>Use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.</br>

```{r bootstrapping}
data <- data.frame(x, y) #Gives the log(body mass) and log(homerange) of dataset
samplerows <- NULL #sampled row number dummy variable
sampledata <- NULL #sampled data dummy
samplecoef <- data.frame()
n = 1000
for (i in 1:1000) { #do this thing 1000 times
     samplerows <- sample(nrow(data), size = n, replace = TRUE) #sample a number corresponding to a row, with replacement to get 1000 rows
    sampledata <- data[samplerows, ] #sample dataframe of the actual values

  m <- lm(y ~ x, data = sampledata) ##Now to fit it to a linear model
  
        
  
  c <- coefficients(m)
  samplecoef[i, 1] <- c[1]
  samplecoef[i, 2] <- c[2]
  #I tried samplecoef <- coeff(m) but it created a dataframe with 0 columns and 1000 rows, had to do it a more rudimentary and longer way
}
colnames(samplecoef) <- c("β0", "β1")
summary(samplecoef) #Let see it. . . it looks right
```
## COMMENT: This looks great! I went a different route for bootstrapping but I like your for loop and it seems to have worked great! Good job :)

</br>Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.</br>
```{r}
#"Make" the population standard deviation funtion AKA copy and paste from my previous HW assignment.
pop_sd <- function(x) {
  sqrt(
    sum((x - mean(x))^2)/(length(x))
  )
} 
#Estimate standard errors of the coefficients
(B0se <- pop_sd(samplecoef$β0))
(B1se <- pop_sd(samplecoef$β1))

#Calculate 95% CI's for the coefficients:
b0ci <- quantile(samplecoef$β0, c(0.025, 0.975))
b1ci <- quantile(samplecoef$β1, c(0.025, 0.975))

b0ci

b1ci

```
 ## COMMENT I like how you tackled this problem. The writing of the function is very cool and sleek! I had some trouble with this part but feel like I understand it a lot better looking at yours.
 
</br>How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?

</br>The SE estimated from the entire dataset using the formula yields a larger number when compared to the SE estimated using the SD of the sample data

</br>How does the latter compare to the 95% CI estimated from your entire dataset?

</br>The 95% CI's estimated of the entire dataset is larger than than CI of the sampled data.